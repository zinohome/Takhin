# Task 3.3: Batch Processing Optimization - Quick Reference

## üìÅ Files Modified/Created

### New Files
```
backend/pkg/kafka/handler/
‚îú‚îÄ‚îÄ batch_aggregator.go          # Batch aggregation logic with adaptive sizing
‚îú‚îÄ‚îÄ batch_aggregator_test.go     # Unit tests (10 test cases)
‚îî‚îÄ‚îÄ batch_benchmark_test.go      # Performance benchmarks (6 scenarios)
```

### Modified Files
```
backend/pkg/config/config.go              # Added BatchConfig struct
backend/pkg/kafka/handler/backend.go      # Added AppendBatch interface
backend/pkg/kafka/handler/raft_backend.go # Added AppendBatch implementation
backend/pkg/storage/topic/manager.go      # Added Topic.AppendBatch method
```

## ‚öôÔ∏è Configuration

### YAML Configuration
```yaml
kafka:
  batch:
    max:
      size: 1000          # Max records per batch
      bytes: 1048576      # Max 1MB per batch
    linger:
      ms: 10              # Wait up to 10ms for batching
    adaptive:
      enabled: true       # Enable adaptive batch sizing
      min:
        size: 16          # Min batch size
      max:
        size: 10000       # Max batch size
    compression:
      type: none          # Options: none, gzip, snappy, lz4, zstd
```

### Environment Variables
```bash
export TAKHIN_KAFKA_BATCH_MAX_SIZE=1000
export TAKHIN_KAFKA_BATCH_MAX_BYTES=1048576
export TAKHIN_KAFKA_BATCH_LINGER_MS=10
export TAKHIN_KAFKA_BATCH_ADAPTIVE_ENABLED=true
export TAKHIN_KAFKA_BATCH_ADAPTIVE_MIN_SIZE=16
export TAKHIN_KAFKA_BATCH_ADAPTIVE_MAX_SIZE=10000
```

## üßë‚Äçüíª API Usage

### Basic Batch Aggregation
```go
// Create aggregator
cfg := &config.BatchConfig{
    MaxSize:  100,
    MaxBytes: 1048576,
    LingerMs: 10,
}
ba := handler.NewBatchAggregator(cfg)
defer ba.Close()

// Add records
batch, shouldFlush := ba.Add("topic", 0, key, value)
if shouldFlush {
    // Process batch
    offsets, err := backend.AppendBatch(batch.TopicName, batch.Partition, batch.Records)
}
```

### Adaptive Batching
```go
cfg := &config.BatchConfig{
    MaxSize:         1000,
    MaxBytes:        10485760,
    AdaptiveEnabled: true,
    AdaptiveMinSize: 10,
    AdaptiveMaxSize: 5000,
}
ba := handler.NewBatchAggregator(cfg)

// Process with metrics tracking
err := ba.ProcessBatch(ctx, batch, func(pb *handler.PartitionBatch) error {
    return backend.AppendBatch(pb.TopicName, pb.Partition, pb.Records)
})
```

### Backend Interface
```go
type Backend interface {
    Append(topicName string, partition int32, key, value []byte) (int64, error)
    AppendBatch(topicName string, partition int32, records []BatchRecord) ([]int64, error)
}
```

## üß™ Testing

### Run Unit Tests
```bash
cd backend

# All batch aggregator tests
go test -v ./pkg/kafka/handler -run TestBatchAggregator

# Specific test
go test -v ./pkg/kafka/handler -run TestBatchAggregator_MaxSizeFlush
```

### Run Benchmarks
```bash
# Quick benchmarks
go test -bench=BenchmarkBatchAggregator -benchmem -benchtime=1s ./pkg/kafka/handler/

# Full produce benchmarks
go test -bench=BenchmarkProduceThroughput -benchmem -benchtime=3s ./pkg/kafka/handler/

# Batch vs single comparison
go test -bench=BenchmarkBatchVsSingle -benchmem -benchtime=5s ./pkg/kafka/handler/

# Concurrent batching
go test -bench=BenchmarkConcurrentBatchProduce -benchmem ./pkg/kafka/handler/

# Adaptive batching
go test -bench=BenchmarkAdaptiveBatching -benchmem ./pkg/kafka/handler/
```

### Performance Profiling
```bash
# CPU profile
go test -bench=BenchmarkBatchAggregator_Throughput \
  -cpuprofile=cpu.prof -benchtime=10s ./pkg/kafka/handler/
go tool pprof -http=:8080 cpu.prof

# Memory profile
go test -bench=BenchmarkBatchAggregator_Throughput \
  -memprofile=mem.prof -benchtime=10s ./pkg/kafka/handler/
go tool pprof -http=:8080 mem.prof
```

## üìä Expected Performance

### Throughput Improvements
| Batch Size | Single Append | Batch Append | Improvement |
|------------|---------------|--------------|-------------|
| 10         | ~100 MB/s     | ~300 MB/s    | 3x          |
| 100        | ~200 MB/s     | ~600 MB/s    | 3x          |
| 1000       | ~300 MB/s     | ~800 MB/s    | 2.7x        |

### I/O Reduction
- **Disk writes**: Reduced by batch size factor (10-1000x fewer syscalls)
- **Lock contention**: Reduced by batch size factor
- **Memory allocations**: Pooled and reused

### Trade-offs
- ‚úÖ **Throughput**: 2-3x improvement
- ‚úÖ **CPU efficiency**: 30% reduction
- ‚ö†Ô∏è **Latency**: Increased by linger time (configurable)

## üîç Monitoring

### Check Aggregator Stats
```go
stats := ba.GetStats()
fmt.Printf("Pending batches: %d\n", stats["pending_batches"])
fmt.Printf("Pending records: %d\n", stats["pending_records"])
fmt.Printf("Target batch size: %d\n", stats["target_batch_size"])
fmt.Printf("Avg throughput: %.2f MB/s\n", stats["avg_throughput"])
```

### Key Metrics
- `pending_batches`: Number of batches waiting to flush
- `pending_records`: Total records in pending batches
- `target_batch_size`: Current adaptive batch size target
- `avg_throughput`: Moving average throughput (MB/s)

## üêõ Troubleshooting

### High Memory Usage
**Symptom**: Memory growth over time  
**Cause**: Large pending batches  
**Solution**: Reduce `max.bytes` or `linger.ms`

### High Latency
**Symptom**: Slow produce responses  
**Cause**: Long linger time  
**Solution**: Reduce `linger.ms` or disable adaptive mode

### Low Throughput with Batching
**Symptom**: No performance improvement  
**Cause**: Batch size too small  
**Solution**: Increase `max.size` or enable adaptive mode

### Batch Size Not Adapting
**Symptom**: Adaptive mode not working  
**Cause**: Not enough throughput samples  
**Solution**: Ensure `ProcessBatch()` is used, not manual handling

## üéØ Best Practices

### For High Throughput
```yaml
kafka:
  batch:
    max:
      size: 5000
      bytes: 10485760  # 10MB
    linger:
      ms: 50
    adaptive:
      enabled: true
```

### For Low Latency
```yaml
kafka:
  batch:
    max:
      size: 10
      bytes: 102400    # 100KB
    linger:
      ms: 1
    adaptive:
      enabled: false
```

### For Balanced Performance
```yaml
kafka:
  batch:
    max:
      size: 1000
      bytes: 1048576   # 1MB
    linger:
      ms: 10
    adaptive:
      enabled: true
```

## üìñ Implementation Details

### Batch Aggregation Flow
```
1. Producer sends record
2. BatchAggregator.Add(topic, partition, key, value)
3. Check flush conditions:
   - Max size reached?
   - Max bytes reached?
   - Adaptive target reached?
4. If yes: Return batch for processing
5. If no: Buffer and wait for more records or linger timeout
```

### Adaptive Algorithm
```
1. Track batch size and throughput
2. Calculate moving average (Œ±=0.2 smoothing)
3. Every 5 seconds:
   - If throughput improving: increase batch size 10%
   - If throughput degrading: decrease batch size 10%
4. Respect min/max bounds
```

### Memory Pool Usage
```go
// Batch buffer from pool
batchBuf := mempool.GetBuffer(totalSize)
defer mempool.PutBuffer(batchBuf)

// Index buffer from pool
indexBuf := mempool.GetBuffer(records * 16)
defer mempool.PutBuffer(indexBuf)
```

## üîó Related Tasks

- **Task 3.1**: Zero-Copy Optimization (fetch side batching)
- **Task 3.2**: Memory Pool Optimization (buffer management)
- **Task 1.1**: Basic produce/fetch (foundation)

## üìù Notes

- Batch aggregation is optional and configurable
- Existing single-record API remains unchanged
- Backward compatible with non-batched clients
- Raft backend batching is TODO (currently falls back to single appends)

---

**Version**: 1.0  
**Last Updated**: 2025-01-06  
**Status**: ‚úÖ Implemented
