package benchmark
// Copyright 2025 Takhin Data, Inc.

package benchmark

import (
	"bytes"
	"encoding/binary"
	"fmt"
	"testing"
	"time"





























































































































































































































































































}	return		p99 = sorted[len(sorted)*99/100]	p95 = sorted[len(sorted)*95/100]	p50 = sorted[len(sorted)*50/100]		}		}			}				sorted[i], sorted[j] = sorted[j], sorted[i]			if sorted[i] > sorted[j] {		for j := i + 1; j < len(sorted); j++ {	for i := 0; i < len(sorted); i++ {	// Bubble sort (acceptable for benchmark data)		copy(sorted, latencies)	sorted := make([]time.Duration, len(latencies))	// Simple percentile calculation (not precise but fast)		}		return 0, 0, 0	if len(latencies) == 0 {func calculatePercentiles(latencies []time.Duration) (p50, p95, p99 time.Duration) {}	return buf.Bytes()		binary.Write(buf, binary.BigEndian, int32(maxBytes)) // Partition max bytes	binary.Write(buf, binary.BigEndian, int64(0))  // Log start offset	binary.Write(buf, binary.BigEndian, offset)    // Fetch offset	binary.Write(buf, binary.BigEndian, partition) // Partition index	binary.Write(buf, binary.BigEndian, int32(1))  // Partition array length	// Partitions		buf.WriteString(topic)	binary.Write(buf, binary.BigEndian, int16(len(topic))) // Topic name length	binary.Write(buf, binary.BigEndian, int32(1))  // Topic array length	// Topics		binary.Write(buf, binary.BigEndian, int8(0))   // Isolation level	binary.Write(buf, binary.BigEndian, int32(maxBytes)) // Max bytes	binary.Write(buf, binary.BigEndian, int32(1024)) // Min bytes	binary.Write(buf, binary.BigEndian, int32(500)) // Max wait ms	binary.Write(buf, binary.BigEndian, int32(-1)) // Replica ID	// Fetch request body		binary.Write(buf, binary.BigEndian, int16(0))  // Client ID length (null)	binary.Write(buf, binary.BigEndian, int32(1))  // Correlation ID	binary.Write(buf, binary.BigEndian, int16(12)) // API Version	binary.Write(buf, binary.BigEndian, int16(1))  // API Key: Fetch	// Request header		buf := new(bytes.Buffer)	// Simplified Fetch request (API Key 1)func createFetchRequest(topic string, partition int32, offset int64, maxBytes int64) []byte {}	return buf.Bytes()		binary.Write(buf, binary.BigEndian, int32(0)) // Headers count	buf.Write(value) // Value	binary.Write(buf, binary.BigEndian, int32(len(value))) // Value length	binary.Write(buf, binary.BigEndian, int32(0)) // Key length (-1 = null)	binary.Write(buf, binary.BigEndian, int32(0)) // Offset delta	binary.Write(buf, binary.BigEndian, int64(0)) // Timestamp delta	binary.Write(buf, binary.BigEndian, int8(0))  // Attributes	binary.Write(buf, binary.BigEndian, int8(len(value))) // Length varint (simplified)	// Single record (simplified)		binary.Write(buf, binary.BigEndian, int32(1))  // Record count	binary.Write(buf, binary.BigEndian, int32(-1)) // Base sequence	binary.Write(buf, binary.BigEndian, int16(-1)) // Producer epoch	binary.Write(buf, binary.BigEndian, int64(-1)) // Producer ID	binary.Write(buf, binary.BigEndian, int64(time.Now().UnixMilli())) // Max timestamp	binary.Write(buf, binary.BigEndian, int64(time.Now().UnixMilli())) // First timestamp	binary.Write(buf, binary.BigEndian, int32(0))  // Last offset delta	binary.Write(buf, binary.BigEndian, int16(0))  // Attributes	binary.Write(buf, binary.BigEndian, int32(0))  // CRC	binary.Write(buf, binary.BigEndian, int8(2))   // Magic	binary.Write(buf, binary.BigEndian, int32(0))  // Partition leader epoch	binary.Write(buf, binary.BigEndian, int32(1))  // Batch length	binary.Write(buf, binary.BigEndian, int64(0))  // Base offset	binary.Write(buf, binary.BigEndian, int32(len(value)+100)) // Record batch size (approx)	// Record batch (simplified)		binary.Write(buf, binary.BigEndian, partition) // Partition index	binary.Write(buf, binary.BigEndian, int32(1))  // Partition array length	// Partition data		buf.WriteString(topic)	binary.Write(buf, binary.BigEndian, int16(len(topic))) // Topic name length	binary.Write(buf, binary.BigEndian, int32(1))  // Topic array length	// Topic data		binary.Write(buf, binary.BigEndian, int32(30000)) // Timeout ms	binary.Write(buf, binary.BigEndian, int16(-1)) // Acks: all	binary.Write(buf, binary.BigEndian, int16(0))  // Transactional ID (null)	// Produce request body (simplified)		binary.Write(buf, binary.BigEndian, int16(0))  // Client ID length (null)	binary.Write(buf, binary.BigEndian, int32(1))  // Correlation ID	binary.Write(buf, binary.BigEndian, int16(9))  // API Version	binary.Write(buf, binary.BigEndian, int16(0))  // API Key: Produce	// Request header (simplified)		buf := new(bytes.Buffer)	// Simplified Produce request (API Key 0)func createProduceRequest(topic string, partition int32, value []byte) []byte {// Helper functions}	}		})			b.ReportMetric(float64(p99.Microseconds()), "p99_us")			b.ReportMetric(float64(p95.Microseconds()), "p95_us")			b.ReportMetric(float64(p50.Microseconds()), "p50_us")						p50, p95, p99 := calculatePercentiles(latencies)			// Calculate statistics						b.StopTimer()						}				}					b.Fatal(err)				if err != nil {								latencies[i] = time.Since(start)				_, err := h.HandleFetch(fetchReq)				start := time.Now()			for i := 0; i < b.N; i++ {						latencies := make([]time.Duration, b.N)						b.ReportAllocs()			b.ResetTimer()						fetchReq := createFetchRequest("benchmark-topic", 0, 0, int64(fetchSize))			// Prepare fetch request						}				require.NoError(b, err)				_, err := h.HandleProduce(req)				req := createProduceRequest("benchmark-topic", 0, value)			for i := 0; i < 1000; i++ {			value := make([]byte, 1024)			// Pre-populate with data						h := handler.New(cfg, mgr)						require.NoError(b, err)			err := mgr.CreateTopic("benchmark-topic", 1)			mgr := topic.NewManager(cfg.Storage.DataDir, cfg.Storage.LogSegmentSize)						}				},					LogSegmentSize: 1024 * 1024 * 1024,					DataDir:        b.TempDir(),				Storage: config.StorageConfig{			cfg := &config.Config{			// Setup		b.Run(name, func(b *testing.B) {		name := fmt.Sprintf("messages=%d", fetchSize)	for _, fetchSize := range fetchSizes {		fetchSizes := []int{1, 10, 100} // Number of messages to fetchfunc BenchmarkFetchLatency(b *testing.B) {// BenchmarkFetchLatency measures end-to-end fetch latency}	}		})			b.ReportMetric(mbPerSec, "MB/s")			b.ReportMetric(messagesPerSec, "msg/s")						mbPerSec := float64(totalMessages*msgSize) / elapsed / 1024 / 1024			messagesPerSec := float64(totalMessages) / elapsed			elapsed := b.Elapsed().Seconds()			// Calculate throughput						b.StopTimer()						}				}					totalMessages++					}						b.Fatal(err)					if err != nil {					_, err := h.HandleProduce(req)				for _, req := range requests {			for i := 0; i < b.N; i++ {			totalMessages := 0						b.ReportAllocs()			b.ResetTimer()						}				requests[i] = createProduceRequest("benchmark-topic", 0, value)			for i := 0; i < batchSize; i++ {			requests := make([][]byte, batchSize)			value := make([]byte, msgSize)			// Prepare batch						h := handler.New(cfg, mgr)						require.NoError(b, err)			err := mgr.CreateTopic("benchmark-topic", 1)			mgr := topic.NewManager(cfg.Storage.DataDir, cfg.Storage.LogSegmentSize)						}				},					LogSegmentSize: 1024 * 1024 * 1024,					DataDir:        b.TempDir(),				Storage: config.StorageConfig{			cfg := &config.Config{			// Setup		b.Run(name, func(b *testing.B) {		name := fmt.Sprintf("batch=%d", batchSize)	for _, batchSize := range batchSizes {		msgSize := 1024	batchSizes := []int{1, 10, 100}func BenchmarkProduceThroughput(b *testing.B) {// BenchmarkProduceThroughput measures messages per second}	}		})			b.ReportMetric(float64(p99.Microseconds()), "p99_us")			b.ReportMetric(float64(p95.Microseconds()), "p95_us")			b.ReportMetric(float64(p50.Microseconds()), "p50_us")						p50, p95, p99 := calculatePercentiles(latencies)			// Calculate statistics						b.StopTimer()						}				}					b.Fatal(err)				if err != nil {								latencies[i] = time.Since(start)				_, err := h.HandleProduce(request)				start := time.Now()			for i := 0; i < b.N; i++ {						latencies := make([]time.Duration, b.N)						b.ReportAllocs()			b.ResetTimer()						request := createProduceRequest("benchmark-topic", 0, value)			value := make([]byte, msgSize)			// Prepare request						h := handler.New(cfg, mgr)						require.NoError(b, err)			err := mgr.CreateTopic("benchmark-topic", 1)			mgr := topic.NewManager(cfg.Storage.DataDir, cfg.Storage.LogSegmentSize)						}				},					LogSegmentSize: 1024 * 1024 * 1024, // 1GB					DataDir:        b.TempDir(),				Storage: config.StorageConfig{			cfg := &config.Config{			// Setup		b.Run(name, func(b *testing.B) {		name := fmt.Sprintf("msgSize=%dB", msgSize)	for _, msgSize := range messageSizes {		messageSizes := []int{100, 1024, 10240} // 100B, 1KB, 10KBfunc BenchmarkProduceLatency(b *testing.B) {// BenchmarkProduceLatency measures end-to-end produce latency)	"github.com/takhin-data/takhin/pkg/storage/topic"	"github.com/takhin-data/takhin/pkg/kafka/handler"	"github.com/takhin-data/takhin/pkg/config"	"github.com/stretchr/testify/require"